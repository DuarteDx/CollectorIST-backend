import path from 'path'
import fs from 'fs'
import { logger } from 'papagaio'
import { getLockProvider, getPersistenceMigrationProvider } from '@/providers'
import { sleep } from '@/utils'
import JSONStream from 'JSONStream'

const MIGRATIONS_FOLDER = 'migrations/jobs'
let dirMainFile
async function run ({ jobsPath, lockProvider, migrationPersistenceProvider }) {
  logger(['info', 'migration'], `Starting migration with ${migrationPersistenceProvider.constructor.name}`)
  let dbVersion = await migrationPersistenceProvider.getDbVersion()
  if (!dbVersion) {
    logger(['info', 'migration'], 'Database is not initialized yet...')
  } else {
    logger(['info', 'migration'], `Database is in version ${dbVersion}`)
  }
  if (fs.existsSync(jobsPath)) {
    for (let filename of fs.readdirSync(jobsPath).sort()) {
      const version = Number(filename.split('-')[0])
      const migration = require(path.join(jobsPath, filename))
      if (!migration.env || migration.env.includes(process.env.NODE_ENV)) {
        if (version > dbVersion) {
          logger(['info', 'migration'], `Running migration ${filename} because ${version} (Job Version) > ${dbVersion} (Current DB Version)`)
          await migration.run()
          await migrationPersistenceProvider.setDbVersion({ dbVersion: version })
          dbVersion = version
        } else {
          logger(['info', 'migration'], `Skipping migration ${filename} because ${version} (Job Version) <= ${dbVersion} (Current DB Version)`)
        }
      }
    }
    logger(['info', 'migration'], `Migration finished successfully`)
  } else {
    logger(['info', 'porg', 'migrations'], 'Porg could not find any migrations folder.')
  }
}

const runMigrations = async ({ lockProviderName, migrationPersistenceProviderName, applicationName, nodeStartPath }) => {
  dirMainFile = path.dirname(require.main.filename)
  if (nodeStartPath) {
    dirMainFile = path.resolve(dirMainFile, nodeStartPath)
  }
  let normalizePath = path.join(dirMainFile, MIGRATIONS_FOLDER)
  let lockProvider = getLockProvider({name: lockProviderName})
  let migrationPersistenceProvider = getPersistenceMigrationProvider({name: migrationPersistenceProviderName})
  const session = await lockProvider.createSession()
  const sessionRenewInterval = setInterval(() => {
    session.renewSession()
  }, 5000)
  while (!(await session.attemptLock({lockId: 'migrations'}))) {
    await sleep(1000)
  }
  await run({jobsPath: normalizePath, lockProvider, migrationPersistenceProvider})
  session.releaseLock({lockId: 'migrations'})
  session.destroySession()
  clearInterval(sessionRenewInterval)
}
const loadFile = function ({ filename, pattern }) {
  let normalizePath = path.join(dirMainFile, 'migrations/data')
  logger(['info', 'migration', 'load-file'], `Environment: ${process.env.NODE_ENV}`)
  logger(['info', 'migration', 'load-file'], `Looking up ${filename}`)
  if (fs.existsSync(`${normalizePath}/${process.env.NODE_ENV}/${filename}`)) {
    logger(['info', 'migration', 'load-file'], `Loading file from ./data/${process.env.NODE_ENV}/${filename}`)
    return require(`${normalizePath}/${process.env.NODE_ENV}/${filename}`)
  } else {
    logger(['info', 'migration', 'load-file'], `Loading file from ./data/common/${filename}`)
    return require(`${normalizePath}/common/${filename}`)
  }
}

const loadFileStream = function ({ filename, pattern }) {
  let normalizePath = path.join(dirMainFile, 'migrations/data')
  logger(['info', 'migration', 'load-file'], `Environment: ${process.env.NODE_ENV}`)
  logger(['info', 'migration', 'load-file'], `Looking up ${filename}`)
  if (fs.existsSync(`${normalizePath}/${process.env.NODE_ENV}/${filename}`)) {
    logger(['info', 'migration', 'load-file'], `Loading file from ./data/${process.env.NODE_ENV}/${filename}`)
    return parseJsonFile({ path: `${normalizePath}/${process.env.NODE_ENV}/${filename}`, pattern })
  } else {
    logger(['info', 'migration', 'load-file'], `Loading file from ./data/common/${filename}`)
    return parseJsonFile({ path: `${normalizePath}/common/${filename}`, pattern })
  }
}

function parseJsonFile ({ path, pattern }) {
  pattern = pattern || '*'
  const stream = fs.createReadStream(path, { encoding: 'utf8' })
  const parser = JSONStream.parse(pattern)
  stream.pipe(parser)
  return parser
}

export {
  runMigrations,
  loadFile,
  loadFileStream
}
